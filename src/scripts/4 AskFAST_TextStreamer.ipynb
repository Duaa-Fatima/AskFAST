{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYFLka5h709J",
        "outputId": "d3e717ed-8727-45ef-fd23-a37b9f6c212f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xformers<0.0.27 in /usr/local/lib/python3.10/dist-packages (0.0.26.post1)\n",
            "Requirement already satisfied: trl<0.9.0 in /usr/local/lib/python3.10/dist-packages (0.8.6)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.12.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da4lxaJX7mzG",
        "outputId": "4d42230d-10d9-4c45-e0ea-218427456b99"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/quantizers/auto.py:167: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
            "  warnings.warn(warning_msg)\n",
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        }
      ],
      "source": [
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    \"ahmedembedded/AskFAST\",\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ahmedembedded/AskFAST\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NTsCNoK18w8i"
      },
      "outputs": [],
      "source": [
        "FAST_prompt_context = \"\"\"You are an admission officer at Fast University Pakistan. Your role is to answer queries related to the admission process at Fast University. You are expected to provide detailed and accurate responses to questions regarding:\n",
        "\n",
        "- Application deadlines\n",
        "- Required documents\n",
        "- Eligibility criteria for different programs\n",
        "- Admission process details\n",
        "- Any other admissions-related information specific to Fast University Pakistan\n",
        "\n",
        "Do not respond to any questions that are not related to admissions at Fast University Pakistan. Maintain a professional and helpful tone, ensuring that prospective students receive the information they need to apply successfully. If there's a question about comparisons, respond \"I'm not a career counselling bot\".\n",
        "\n",
        "**Example Questions:**\n",
        "\n",
        "1. What is the application deadline for the upcoming semester?\n",
        "2. What documents are required for the application?\n",
        "3. What are the eligibility criteria for the Computer Science program?\n",
        "4. How competitive is the admission process for the Business Administration program?\n",
        "\n",
        "Stay focused on admissions-related topics only.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HHFweVel8x2K"
      },
      "outputs": [],
      "source": [
        "past_prompts = []\n",
        "\n",
        "def get_answer(question: str) -> str:\n",
        "    if len(past_prompts) >= 10:\n",
        "        past_prompts.pop(0)\n",
        "\n",
        "    past_prompts.append(f\"User: {question}\")\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        [\n",
        "            FAST_prompt_context.format(\n",
        "                past_prompts,\n",
        "                question,\n",
        "                \"\",\n",
        "            )\n",
        "        ], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    response = model.generate(**inputs, max_new_tokens=128)\n",
        "\n",
        "    response_text = tokenizer.decode(response[0], skip_special_tokens=True)\n",
        "\n",
        "    past_prompts.append(question)\n",
        "    past_prompts.append(response_text.split('Response:')[1].split('### Input:')[0])\n",
        "\n",
        "    return response_text.split('Response:')[1].split('### Input:')[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "rR-7iNA59Pmw",
        "outputId": "359cb9c1-6516-4030-e09b-5d0f722a5397"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nI'm sorry, I don't have the information on Lahore's weather. You may want to check local news reports or websites dedicated to weather information.\""
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_answer(\"How's Lahore weather?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "gEbaviRU9so5",
        "outputId": "7222d933-8869-45e6-9754-180f1086c7ee"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'subprocess' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-5f57090cd9f6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'install'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'anvil-uplink'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0manvil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0manvil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmpl_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'subprocess' is not defined"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "subprocess.run(['pip', 'install', 'anvil-uplink'])\n",
        "import anvil.server\n",
        "import anvil.mpl_util"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIXMQXgb9SOF"
      },
      "outputs": [],
      "source": [
        "anvil.server.connect(\"server_4A4U4BE7XQDAEMCUOC3JW7ZY-5ACGOW4HWMXBB2OB\")\n",
        "prompts_daily = [12, 45, 80, 1]\n",
        "\n",
        "@anvil.server.callable\n",
        "def get_response_from_model(input_text):\n",
        "    global prompts_daily\n",
        "    prompts_daily[-1] += 1\n",
        "    return get_answer(input_text)\n",
        "\n",
        "@anvil.server.callable\n",
        "def make_plot():\n",
        "    global prompts_daily\n",
        "    plt.figure(figsize=(20, 5), facecolor='white')\n",
        "    plt.bar(range(len(prompts_daily)), prompts_daily, color='#668bd6')\n",
        "    plt.xlabel('Index')\n",
        "    plt.ylabel('Value')\n",
        "    plt.title('Prompts Daily')\n",
        "    plt.gca().set_facecolor('white')\n",
        "    return anvil.mpl_util.plot_image()\n",
        "\n",
        "anvil.server.wait_forever()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
